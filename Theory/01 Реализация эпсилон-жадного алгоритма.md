# Реализация эпсилон-жадного алгоритма (Epsilon-Greedy)


## Конструктор класса

Сначала определим Класс, который представляет алгоритм **Epsilon-Greedy** в том виде, в котором он будет использоваться в реальной жизни. Этот класс будет содержать следующую информацию:

- **epsilon**
    - Это число с плавающей точкой, которое определяет частоту, с которой мы должны исследовать одну из доступных рук. Если мы установим epsilon = 0.1, то мы будем исследовать доступные руки только в 10 % случаев.

- **counts**
    - Вектор целых чисел длины N, который говорит о том, сколько раз мы сыграли каждой из N рук, доступных нам в текущей задаче с бандитом. Если есть две руки, рука 1 и рука 2, каждой из которых сыграли 2 раза, то зададим counts = [2, 2].

- **values**
    - Вектор чисел с плавающей точкой, определяющий среднее количество вознаграждения, которое мы получили, играя каждой из N доступных нам рук. Если рука 1 дала нам 1 единицу награды в одной игре и 0 в другой, а рука 2 дала нам 0 единиц награды в обеих играх, то мы зададим значения = [0.5, 0.0].

Собрав все эти части вместе в правильное определение класса, мы получим следующий фрагмент кода:

```python
class EpsilonGreedy:
    """## Эпсилон-жадный алгоритм"""

    def __init__(self, epsilon: float, counts: list[int], values: list[float]):
        """## Конструктор

        ### Args:
            - `epsilon (float)`: Коэффициент исследования
            - `counts (list)`: Вектор - сколько раз сыграли каждой из рук
            - `values (list)`: Вектор с вознаграждениями
        """
        self.epsilon = epsilon
        self.counts = counts
        self.values = values
```

### initialize

Поскольку поведение алгоритма **Epsilon-Greedy** очень сильно зависит от настроек счетчиков и значений, мы также предоставляем явные методы инициализации, которые позволяют вам вернуть эти переменные в состояние, соответствующее "чистому":

```python
def initialize(self, n_arms):
    self.counts = [0 for col in range(n_arms)]
    self.values = [0.0 for col in range(n_arms)]
    return
```

### select_arm и update

Теперь, когда у нас есть класс, представляющий всю информацию, которую алгоритм epsilon-Greedy должен хранить о каждой из рук, нам нужно определить два метода, которые должны быть у любой алгоритм для решения проблемы многорукого бандита:

- **select_arm**
    - Каждый раз, когда нам нужно сделать выбор, какую руку тянуть, мы хотим иметь возможность просто вызвать алгоритм, чтобы он сообщил нам порядковый номер руки, которую мы должны тянуть. Все алгоритмы бандитов будут реализовывать метод **select_arm**, который вызывается без каких-либо аргументов и возвращает индекс следующей руки, которую нужно потянуть.
    
- **update** 
    - После того как мы потянем за руку, мы получим от нашей среды сигнал о вознаграждении. Мы хотим обновить информацию о качестве руки, которую мы только что выбрали, предоставив бандиту информацию о вознаграждении. 

Все алгоритмы бандитов делают это, предоставляя метод **update**, которая принимает в качестве аргументов (1) объект алгоритма, (2) числовой индекс последней выбранной руки и (3) вознаграждение, полученное при выборе этой руки. Метод обновления получает эту информацию и вносит соответствующие изменения в оценку алгоритмом всех рук.

Помня об общей структуре поведения, которую мы ожидаем от алгоритма бандита, давайте рассмотрим конкретное определение этих двух методов для алгоритма **Epsilon-Greedy**. 

### select_arm в Epsilon-Greedy

Сначала реализуем **select_arm**:

```python
@staticmethod
def ind_max(x: list) -> int:
    m = max(x)
    return x.index(m)

def select_arm(self) -> int:
    """## Выбор руки

    ### Returns:
        - `int`: Возвращает руку
    """
    if random.random() > self.epsilon:
        return EpsilonGreedy.ind_max(self.values)
    else:
        return random.randrange(len(self.values))
```

Как видите, алгоритм **Epsilon-Greedy** определяет руку следующим образом: 
- мы бросаем монету, чтобы проверить, выберем ли мы лучшую руку, о которой знаем,
- а затем если монета выпадает решкой, мы выбираем руку совершенно случайно. 

В Python мы реализовали это, проверив, не превышает ли случайно сгенерированное число значение эпсилон. Если да, то наш алгоритм выбирает руку, чье кэшированное значение в соответствии с полем значений больше; в противном случае он выбирает руку случайным образом.

Эти несколько строк кода полностью описывают решение задачи бандита алгоритмом **Epsilon-Greedy**: он исследует некоторый процент времени, а в остальное время выбирает руку, которую считает лучшей. Но чтобы понять, какую руку наш алгоритм **Epsilon-Greedy** считает лучшей, нам нужно определить метод **update**. 

### update в Epsilon-Greedy

Давайте определим метод **update** сейчас, а затем объясним, почему выбранная нами процедура является разумной:

```python
def update(self, chosen_arm: int, reward: int) -> None:
    """## Обновление бандита

    ### Args:
        - `chosen_arm (int)`: Выбранная рука
        - `reward (int)`: Награда
    """
    # обновили каунты
    self.counts[chosen_arm] = self.counts[chosen_arm] + 1
    n = self.counts[chosen_arm]

    # обновили значения
    value = self.values[chosen_arm]
    new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward
    self.values[chosen_arm] = new_value
```

- Взглянув на этот код, мы увидим, что метод **update** сначала увеличивает значение в списке counts, в котором записано количество раз, когда мы играли каждой из рук. 
- Если рукой еще не играли, то мы устанавливаем в качестве значения непосредственно вознаграждение, которое мы только что получили от игры на этой руке. 
- Если мы уже играли с этой рукой в прошлом, мы обновляем оценочную стоимость выбранной руки, чтобы она была средневзвешенным значением ранее оцененной стоимости и награды, которую мы только что получили. 
- Это взвешивание важно, потому что оно означает, что отдельные наблюдения значат для алгоритма все меньше и меньше, когда у нас уже есть большой опыт работы с каким-либо конкретным вариантом. Выбранное нами взвешивание призвано гарантировать, что расчетная стоимость будет точно равна среднему значению вознаграждения, полученного от каждой руки.

**Уточнение к new_value**: 
В коде есть следующая строка: 
```python
new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward
```
Она позволяет не хранить суммарную награду. В values хранятся следующие значения для каждой из рук: суммарная награда, полученная от руки, деленная на количестов раз, которое эту руку бандит выбрал. В counts для каждой из рук хранятся значения - количество раз, которое бандит выбрал руку.

Пример того, как интуитивно работает бандит с обновлением значений:

Ответ от среды 1: count=1 -> sum_reward=1 -> value=1
Ответ от среды 0: count=2 -> sum_reward=1 -> value=0.5
Ответ от среды 0: count=3 -> sum_reward=1 -> value=0.333
Ответ от среды 1: count=4 -> sum_reward=2 -> value=0.5
Ответ от среды 0: count=5 -> sum_reward=2 -> value=0.4

В итоге, чтобы из предыдущего значения value получить сумму, нужно это value домножить на (n-1). Таким образом, можно просто к этой сумме добавить новое значение и получить новое значение value.

$new\_value = \frac{n-1}{n}\cdot value + \frac{1}{n} \cdot reward$

$value_1 = \frac{1-1}{1}\cdot 0 + \frac{1}{1} \cdot 1 = 1$
$value_2 = \frac{2-1}{2}\cdot 1 + \frac{1}{2} \cdot 0 = 0.5$
$value_3 = \frac{3-1}{3}\cdot 0.5 + \frac{1}{3} \cdot 0 = 0.33$
$value_4 = \frac{4-1}{4}\cdot 0.33 + \frac{1}{4} \cdot 1 = 0.5$
$value_5 = \frac{5-1}{5}\cdot 0.5 + \frac{1}{5} \cdot 0 = 0.4$

**Замечание**: Мы говорим о том, как вычислять средние значения в режиме онлайн, потому что на практике поведение бандитских алгоритмов во многом определяется этим правилом вычисления средних значений. Далее мы поговорим об альтернативных схемах взвешивания, которые вы можете использовать вместо вычисления средних. Эти альтернативные схемы взвешивания очень важны, когда руки, на которых вы играете, могут менять свое вознаграждение с течением времени.

### Полный class EpsilonGreedy

Но сейчас давайте сосредоточимся на том, что мы сделали до сих пор. Определение класса EpsilonGreedy и определение методов **select_arm** и **update** для этого класса полностью определяют нашу реализацию алгоритма **Epsilon-Greedy**.

```python
class EpsilonGreedy:
    """## Эпсилон-жадный алгоритм"""

    def __init__(self, epsilon: float, counts: list[int], values: list[float]):
        """## Конструктор

        ### Args:
            - `epsilon (float)`: Коэффициент исследования
            - `counts (list)`: Вектор - сколько раз сыграли каждой из рук
            - `values (list)`: Вектор с вознаграждениями
        """
        self.epsilon = epsilon
        self.counts = counts
        self.values = values

    def initialize(self, n_arms: int) -> None:
        """## Инициализация

        ### Args:
            - `n_arms (int)`: Количество рук
        """
        self.counts = [0 for col in range(n_arms)]
        self.values = [0.0 for col in range(n_arms)]

    @staticmethod
    def ind_max(x: list) -> int:
        m = max(x)
        return x.index(m)

    def select_arm(self) -> int:
        """## Выбор руки

        ### Returns:
            - `int`: Возвращает руку
        """
        if random.random() > self.epsilon:
            return EpsilonGreedy.ind_max(self.values)
        else:
            return random.randrange(len(self.values))

    def update(self, chosen_arm: int, reward: int) -> None:
        """## Обновление бандита

        ### Args:
            - `chosen_arm (int)`: Выбранная рука
            - `reward (int)`: Награда
        """
        self.counts[chosen_arm] = self.counts[chosen_arm] + 1
        n = self.counts[chosen_arm]

        value = self.values[chosen_arm]
        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward
        self.values[chosen_arm] = new_value
```

Если мы хотим настроить алгоритм на выбор абсолютно случайным образом (аналог A/B теста):

```python
algo = EpsilonGreedy(epsilon=1.0, counts=[], values=[])
algo.initialize(n_arms=2)
```

Так как epsilon=1.0, то в методе **select_arm** алгоритм всегда пойдет по условию else. То есть исследование 100%.

Настройка на максимизацию прибыли:

```python
algo = EpsilonGreedy(epsilon=0.0, counts=[], values=[])
algo.initialize(n_arms=2)
```

Так как epsilon=0.0, то в методе **select_arm** алгоритм всегда пойдет по первой части условия. То есть исследование 0%. Всегда выбирается рука с максимальной суммой награды.

Эти четыре строкикода полностью описывают варианты, которые буквально являются двумя самыми крайними случаями, которыми можно настроить алгоритм **Epsilon-Greedy**.

### Задача логотипа на сайте

В контексте задачи выбора лучшего логотипа на сайте:

- **epsilon = 1.0**
    - Если мы зададим epsilon = 1.0, алгоритм всегда будет выбирать между разными руками совершенно случайным образом. В результате вы получите много данных об обоих цветовых логотипах и очень чистые данные, потому что все руки будут иметь равное количество данных и не будет никаких скрытых помех, которые мешают понять, почему вы получили те результаты, которые получили. 
    
    Если вы традиционно образованный ученый, такой тип случайных экспериментов покажется вам отличным подходом. Но если вы занимаетесь бизнесом, то можете потерять много денег, потому что это означает, что вы с такой же вероятностью попробуете плохую идею, как и хорошую.
    - Или, говоря иначе: если вы управляете бизнесом, вам не стоит накапливать много данных о плохих вариантах. Вы должны собирать данные только о тех вариантах, которые, возможно, стоит реализовать. В этом и заключалась критика: установив epsilon = 1.0, вы тратите ресурсы на сбор данных о плохих вариантах. Возвращаясь к нашему примеру с логотипами, если вы выбираете один из двух цветов совершенно наугад, вы решили, что будете показывать клиентам свой худший логотип ровно столько же раз, сколько и лучший.
    - **Пример**: представьте, что у вас есть хороший логотип и плохой логотип и 4000 пользователей. В итоге, при epsilon = 1.0 вы 2000 пользователей покажете хороший логотип, а 2000 плохой логотип. В итоге, зачем 2000 пользователей показывать плохой логотип? Именно эту проблему и решает бандит: например, 3500 покажем хороший логотип и 1500 плохой.
    
- **epsilon = 0.0**
    - Если вы в конце концов установите epsilon = 0.0, то действительно перестанете тратить время на плохие варианты. Но вы больше никогда не сможете узнавать о новых вариантах. Если мир меняется, а вы не предоставляете своей компании никакого механизма для изучения изменений в мире, ваша компания останется позади.
    
К счастью, нет никаких причин действовать в одной из этих двух крайностей. Вместо того чтобы переходить от одного периода совершенно случайных экспериментов к другому периоду абсолютно жадного выбора так называемого лучшего варианта, алгоритм **Epsilon-Greedy** позволяет вам действовать более постепенно. Например, вы можете установить epsilon = 0.1 и оставить алгоритм работать вечно. Во многих ситуациях это может быть лучшим вариантом.

### Слабые стороны

- Первая слабость заключается в том, что по мере того, как вы все больше убеждаетесь в том, какой из двух дизайнов логотипа лучше, тенденция исследовать худший дизайн целых 5 % времени будет становиться все более расточительной. На жаргоне бандитских алгоритмов, вы будете слишком много исследовать. 
- И есть еще одна проблема с фиксированным правилом исследования в 10 %: в начале экспериментов вы будете выбирать варианты, о которых мало что знаете, гораздо реже, чем вам хотелось бы, потому что вы пробуете новые варианты только в 10 % случаев.

Таким образом, существует множество способов улучшить алгоритм **Epsilon-Greedy**.

