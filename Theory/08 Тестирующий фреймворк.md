# Тестирующий фреймворк

Теперь мы почти готовы начать экспериментировать с алгоритмом **Epsilon-Greedy**. Но перед этим мы создадим общий фреймворк для тестирования алгоритма. Этот фреймворк полностью описывается функцией test_algorithm, показанной ниже, и будет единственным инструментом тестирования. Давайте пройдемся по нему сейчас:

```python
def test_algorithm(
        algo: BaseAlgo,
        arms: list[BaseArm],
        num_sims: int,
        horizon: int) -> tuple[list[int], list[int], list[int], list[float], list[float]]:
    """## Тестовый фреймворк (модель среды)

    ### Args:
        - `algo (BaseAlgo)`: Алгоритм бандита
        - `arms (list[BaseArm])`: Список рук
        - `num_sims (int)`: Количество симуляций
        - `horizon (int)`: Горизонт - количество игр бандитом

    ### Returns:
        - `_type_`: _description_
    """

    chosen_arms = [0 for i in range(num_sims * horizon)]
    rewards = [0.0 for i in range(num_sims * horizon)]
    cumulative_rewards = [0.0 for i in range(num_sims * horizon)]
    sim_nums = [0 for i in range(num_sims * horizon)]
    times = [0 for i in range(num_sims * horizon)]

    for sim in range(num_sims):
        sim = sim + 1
        algo.initialize(len(arms))

        for t in range(horizon):
            t = t + 1
            index = (sim - 1) * horizon + t - 1
            sim_nums[index] = sim
            times[index] = t

            chosen_arm = algo.select_arm()
            chosen_arms[index] = chosen_arm

            reward = arms[chosen_arms[index]].draw()
            rewards[index] = reward

            if t == 1:
                cumulative_rewards[index] = reward
            else:
                cumulative_rewards[index] = cumulative_rewards[index - 1] + reward

            algo.update(chosen_arm, reward)

    return (sim_nums, times, chosen_arms, rewards, cumulative_rewards)
```

## Как работает этот фреймворк? 

- Мы передаем несколько объектов: 
    - алгоритм бандита, который мы хотим протестировать; 
    - массив рук, из которых мы хотим смоделировать розыгрыши; 
    - фиксированное количество симуляций для усреднения шума в каждой симуляции; 
    - количество раз, которое каждому алгоритму разрешено перебирать руки во время каждой симуляции. Любой не самый плохой алгоритм в конце концов узнает, какая рука лучше; интереснее всего изучить в симуляции, хорошо ли работает алгоритм, когда у него есть всего 100 (или 100 000) попыток найти лучшую руку.

- Затем фреймворк использует эти объекты для проведения множества независимых симуляций. Для каждой из них он:
    - Инициализирует настройки алгоритма бандита с нуля, чтобы у него не было предварительных знаний о том, какая рука лучше. 
    - Перебирает возможности потянуть руку. На каждом шаге этого цикла он: 
        - Вызывает **select_arm**, чтобы увидеть, какую руку выберет алгоритм; 
        - Вызывает **draw** на этой руке, чтобы смоделировать результат вытягивания этой руки; 
        - Записывает сумму вознаграждения, полученную алгоритмом, а затем вызывает **update**, чтобы позволить алгоритму обработать эту новую часть информации.

- Наконец, система тестирования возвращает набор данных, который сообщает нам для каждой симуляции, какая рука была выбрана и насколько хорошо работал алгоритм в каждый момент времени.